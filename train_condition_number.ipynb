{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_1 = 100\n",
    "lambda_2 = 1\n",
    "alpha = 1.0\n",
    "delta = 1e-3\n",
    "rho = 50\n",
    "max_iter = 100\n",
    "Z_inner_iter = 10\n",
    "W_lr = None  # 不需要lr, W有闭式解\n",
    "Z_lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1926056/350791921.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  x = torch.load(load_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([131072, 60])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_path = \"/data/user/tourist/mixed-percision-quantization-for-SwinIR/scripts/x.pt\"\n",
    "x = torch.load(load_path)\n",
    "x.shape\n",
    "x = x.reshape(-1, x.shape[-1])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1926056/4040728203.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weight = torch.load(loadd_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 60])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadd_path = \"/data/user/tourist/mixed-percision-quantization-for-SwinIR/scripts/weight.pt\"\n",
    "weight = torch.load(loadd_path)\n",
    "weight = weight.T\n",
    "weight = weight[:, :60]\n",
    "\n",
    "weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([131072, 60])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "X_reshape = x.to(device)\n",
    "Y_reshape = X_reshape@weight.to(device)\n",
    "Y_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9236, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([4.1079, 3.9695, 3.4316, 3.0956, 3.0265, 2.8834, 2.8323, 2.8125, 2.4913,\n",
       "        2.4582, 2.3853, 2.2984, 1.9307, 1.8277, 1.5488, 1.4096, 1.3564, 1.1351,\n",
       "        0.9633, 0.8095, 0.7105, 0.6020, 0.5783, 0.5202, 0.4558, 0.4431, 0.3887,\n",
       "        0.3753, 0.3493, 0.3165, 0.3075, 0.2783, 0.2654, 0.2466, 0.2341, 0.2206,\n",
       "        0.2056, 0.1960, 0.1908, 0.1823, 0.1719, 0.1472, 0.1340, 0.1323, 0.1262,\n",
       "        0.1165, 0.1066, 0.1001, 0.0892, 0.0781, 0.0694, 0.0629, 0.0525, 0.0481,\n",
       "        0.0400, 0.0337, 0.0252, 0.0233, 0.0130, 0.0062], device='cuda:0',\n",
       "       grad_fn=<LinalgSvdBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_, S_, Vh_ = torch.linalg.svd(weight, full_matrices=False)\n",
    "alpha = torch.mean(S_)\n",
    "print(alpha)\n",
    "S_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#W = torch.zeros(weight.shape, device=device)\n",
    "W = weight.clone().detach()\n",
    "Z = W.clone().detach()\n",
    "U = torch.zeros_like(W)\n",
    "\n",
    "XTX = X_reshape.t() @ X_reshape\n",
    "I_n = torch.eye(X_reshape.shape[1], device=device)\n",
    "A = XTX + rho*I_n\n",
    "A_inv = torch.inverse(A)\n",
    "XTY = X_reshape.t() @ Y_reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(65.8947, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([9.2088e+02, 7.2107e+02, 5.8632e+02, 3.2860e+02, 1.8841e+02, 1.8575e+02,\n",
       "        1.6451e+02, 1.1080e+02, 1.0070e+02, 8.8202e+01, 7.4748e+01, 7.0583e+01,\n",
       "        5.9892e+01, 5.9566e+01, 5.2480e+01, 4.9497e+01, 3.3651e+01, 3.3322e+01,\n",
       "        2.8065e+01, 2.4262e+01, 1.4341e+01, 1.1784e+01, 9.7186e+00, 8.1706e+00,\n",
       "        7.4338e+00, 6.4441e+00, 5.2315e+00, 4.4822e+00, 4.1969e+00, 5.6869e-01,\n",
       "        4.8679e-05, 4.2400e-05, 3.9042e-05, 3.6825e-05, 3.2521e-05, 3.1018e-05,\n",
       "        2.9560e-05, 2.9429e-05, 2.8422e-05, 2.6960e-05, 2.6665e-05, 2.5685e-05,\n",
       "        2.4636e-05, 2.4361e-05, 2.4240e-05, 2.3886e-05, 2.3678e-05, 2.3378e-05,\n",
       "        2.2465e-05, 2.2207e-05, 2.1870e-05, 2.1510e-05, 2.1066e-05, 2.0858e-05,\n",
       "        2.0282e-05, 1.9446e-05, 1.8727e-05, 1.7344e-05, 7.8976e-06, 6.5528e-06],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_, S_, Vh_ = torch.linalg.svd(X_reshape, full_matrices=False)\n",
    "alpha = torch.mean(S_)\n",
    "print(alpha)\n",
    "S_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res: 0.001256\n",
      "Iter 0, primal_res: 5.821691\n",
      "res: 0.001217\n",
      "Iter 10, primal_res: 4.555047\n",
      "res: 0.001222\n",
      "Iter 20, primal_res: 4.497575\n",
      "res: 0.001226\n",
      "Iter 30, primal_res: 4.523546\n",
      "res: 0.001231\n",
      "Iter 40, primal_res: 4.531183\n",
      "res: 0.001234\n",
      "Iter 50, primal_res: 4.535338\n",
      "res: 0.001239\n",
      "Iter 60, primal_res: 4.537772\n",
      "res: 0.001241\n",
      "Iter 70, primal_res: 4.539563\n",
      "res: 0.001248\n",
      "Iter 80, primal_res: 4.541033\n",
      "res: 0.001249\n",
      "Iter 90, primal_res: 4.541514\n",
      "res: 0.001251\n",
      "Iter 100, primal_res: 4.541873\n"
     ]
    }
   ],
   "source": [
    "r = torch.tensor(60)\n",
    "r_10 = int(torch.ceil(0.1*r).item())\n",
    "r_20 = int(torch.ceil(0.2*r).item())\n",
    "r_80 = int(torch.ceil(0.8*r).item())\n",
    "r_90 = int(torch.ceil(0.9*r).item())\n",
    "w = torch.ones(r)\n",
    "# 前10%\n",
    "w[:r_10] = 100\n",
    "# 前10%-20%\n",
    "w[r_10:r_20] = 1\n",
    "# 中间60%已经是1了\n",
    "# 后20%-10%\n",
    "w[r_80:r_90] = 1\n",
    "# 最后10%\n",
    "w[r_90:] = 1\n",
    "#w = torch.ones(r)\n",
    "\n",
    "w = w.to(device)\n",
    "for it in range(101):\n",
    "    # W-update\n",
    "    RHS = XTY + rho*(Z - U)\n",
    "    W = A_inv @ RHS\n",
    "\n",
    "    # Z-update\n",
    "    W_plus_U = W + U\n",
    "    #W_plus_U = W\n",
    "    P, sigma_W, Qt = torch.linalg.svd(W_plus_U, full_matrices=False)\n",
    "    r = len(sigma_W)\n",
    "\n",
    "\n",
    "    # Compute target singular value t\n",
    "    #t = torch.mean(sigma_W)\n",
    "    t = 2\n",
    "\n",
    "\n",
    "    # Update singular values\n",
    "    sigma_Z = (rho * sigma_W + lambda_1 * t * w) / (rho + lambda_1*w)\n",
    "    sigma_Z = torch.maximum(sigma_Z, torch.tensor(0))\n",
    "\n",
    "    # Reconstruct Z\n",
    "    Z = P @ torch.diag(sigma_Z) @ Qt\n",
    "\n",
    "\n",
    "    # U-update (scaled dual variable)\n",
    "    U = U + (W - Z)\n",
    "\n",
    "    # 检查收敛\n",
    "    if it % 10 == 0:\n",
    "        res = F.mse_loss(X_reshape @ W, Y_reshape)\n",
    "        primal_res = torch.norm(W - Z, p='fro')\n",
    "        print(f\"res: {res.item():.6f}\")\n",
    "        print(f\"Iter {it}, primal_res: {primal_res.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(598.4934, device='cuda:0', grad_fn=<AddBackward0>) tensor(150., device='cuda:0')\n",
      "tensor(379.4328, device='cuda:0', grad_fn=<MulBackward0>) tensor(219.0606, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(7.5887, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor(2.1906, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "(rho * sigma_W + lambda_1 * t * w) / (rho + lambda_1*w)\n",
    "print(rho * sigma_W[0] + lambda_1 * t * w[0], rho + lambda_1*w[0])\n",
    "print(rho * sigma_W[0], lambda_1 * t * w[0])\n",
    "print(sigma_W[0])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.8087, 3.4180, 2.7875, 2.7127, 2.6968, 2.6278, 2.4946, 2.3875, 2.3811,\n",
       "        2.2992, 2.2859, 2.2371, 2.0016, 2.0013, 2.0011, 2.0009, 2.0008, 2.0008,\n",
       "        2.0007, 2.0005, 2.0005, 2.0004, 2.0004, 2.0003, 2.0003, 2.0002, 2.0002,\n",
       "        2.0001, 2.0000, 2.0000, 1.9999, 1.9999, 1.9998, 1.9998, 1.9997, 1.9996,\n",
       "        1.9995, 1.9995, 1.9993, 1.9992, 1.9991, 1.9989, 1.9939, 1.8920, 1.8567,\n",
       "        1.7581, 1.7361, 1.6618, 1.6232, 1.5935, 1.5556, 1.5086, 1.2935, 1.2035,\n",
       "        1.0730, 0.9908, 0.8997, 0.6623, 0.5918, 0.5269], device='cuda:0',\n",
       "       grad_fn=<LinalgSvdBackward0>)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_, S_, Vh_ = torch.linalg.svd(W, full_matrices=False)\n",
    "S_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def draw_plot(weight):\n",
    "    #weight = torch.abs(weight)\n",
    "    if weight.is_cuda:\n",
    "        weight = weight.detach().cpu().numpy()\n",
    "    else:\n",
    "        weight = weight.numpy()\n",
    "\n",
    "    input_channels = weight.shape[0]\n",
    "    output_channels = weight.shape[1]\n",
    "\n",
    "    X = np.arange(output_channels)\n",
    "    Y = np.arange(input_channels)\n",
    "    X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "    fig = plt.figure(dpi=100)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    top_1_percent_threshold = np.percentile(weight, 99)\n",
    "    # 创建颜色数组，初始化为蓝色，shape 为 (n, 4) 表示 RGBA 值\n",
    "    colors = np.zeros((weight.size, 4))\n",
    "\n",
    "    colors[:] = [0, 0, 1, 0.3]  # 蓝色\n",
    "\n",
    "    top_1_indices = weight.ravel() >= top_1_percent_threshold\n",
    "    colors[top_1_indices] = [1, 0, 0, 0.8]  # 红色\n",
    "\n",
    "    ax.bar3d(X.ravel(), Y.ravel(), np.zeros_like(X.ravel()), 0.2, 0.2, weight.ravel(), color=colors)\n",
    "\n",
    "    ax.set_xlabel('Channel')\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_zlabel('Magnitude')\n",
    "\n",
    "    ax.grid(True)\n",
    "    ax.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "    ax.tick_params(axis='y', which='both', bottom=False, top=False, labelleft=False)\n",
    "\n",
    "\n",
    "\n",
    "    ax.text2D(0.05, 0.95, \"X\", transform=ax.transAxes, fontsize=15, weight='bold')\n",
    "    plt.title(f'input_sampled', fontsize=12)\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(weight)\n",
    "torch.min(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(W)\n",
    "torch.min(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logdet_grad(Z, lambda_1, delta):\n",
    "    Q = Z.t() @ Z + delta*torch.eye(Z.shape[1], device=Z.device)\n",
    "    Q_inv = torch.inverse(Q)\n",
    "    return -2*lambda_1 * Z @ Q_inv\n",
    "\n",
    "def sigma_penalty_grad(Z, lambda_2, alpha):\n",
    "    U_, S_, Vh_ = torch.linalg.svd(Z, full_matrices=False)\n",
    "    diff = S_ - alpha\n",
    "    S_grad = 2 * diff\n",
    "    return lambda_2 * (U_ @ torch.diag(S_grad) @ Vh_)\n",
    "\n",
    "for it in range(81):\n",
    "    # W-update\n",
    "    RHS = XTY + rho*(Z - U)\n",
    "    W = A_inv @ RHS\n",
    "\n",
    "    # Z-update with inner iteration\n",
    "    R = W + U\n",
    "    for _ in range(Z_inner_iter):\n",
    "        grad_logdet = logdet_grad(Z, lambda_1, delta)\n",
    "        grad_sigma = sigma_penalty_grad(Z, lambda_2, alpha)\n",
    "        grad_quad = rho*(Z - R)\n",
    "        grad_Z = grad_logdet + grad_sigma + grad_quad\n",
    "        Z = Z - Z_lr * grad_Z\n",
    "\n",
    "    # U-update (scaled dual variable)\n",
    "    U = U + (W - Z)\n",
    "\n",
    "    # 检查收敛\n",
    "    if it % 10 == 0:\n",
    "        res = F.mse_loss(X_reshape @ W, Y_reshape)\n",
    "        primal_res = torch.norm(W - Z, p='fro')\n",
    "        print(f\"res: {res.item():.6f}\")\n",
    "        print(f\"Iter {it}, primal_res: {primal_res.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prox_nuclear_norm(Z, tau):\n",
    "    U_, S_, Vh_ = torch.linalg.svd(Z, full_matrices=False)\n",
    "    S_thresholded = torch.clamp(S_ - tau, min=0)\n",
    "    return U_ @ torch.diag(S_thresholded) @ Vh_\n",
    "\n",
    "def logdet_grad(Z, delta, lambda_1):\n",
    "    n = Z.shape[1]\n",
    "    I_n = torch.eye(n, device=Z.device)\n",
    "    Q = Z.t() @ Z + delta * I_n\n",
    "    Q_inv = torch.inverse(Q)\n",
    "    grad = -2 * lambda_1 * Z @ Q_inv\n",
    "    return grad\n",
    "\n",
    "def Z_subproblem_update(W, U, lambda_1, lambda_2, rho, Z_init, delta=1e-3, lr=1e-3, inner_iter=10):\n",
    "    Z_ = Z_init.clone()\n",
    "    R = W + U  \n",
    "    I_n = torch.eye(Z_.shape[1], device=Z_.device)\n",
    "    for _ in range(inner_iter):\n",
    "        # grad from logdet\n",
    "        grad_logdet = logdet_grad(Z_, delta, lambda_1)\n",
    "        # grad from quadratic\n",
    "        grad_quad = rho*(Z_ - R)\n",
    "        grad = grad_logdet + grad_quad\n",
    "        Z_temp = Z_ - lr * grad\n",
    "        Z_ = prox_nuclear_norm(Z_temp, lr * lambda_2)\n",
    "    return Z_\n",
    "\n",
    "def conjugate_gradient(Av_func, b, x0=None, tol=1e-6, max_iter=1000, verbose=False):\n",
    "    if b.dim() != 1:\n",
    "        raise ValueError(\"b.dim() != 1\")\n",
    "    x = torch.zeros_like(b) if x0 is None else x0.clone()\n",
    "    r = b - Av_func(x)\n",
    "    p = r.clone()\n",
    "    rr_old = torch.dot(r, r)\n",
    "    initial_norm = torch.sqrt(rr_old)\n",
    "    if initial_norm < tol:\n",
    "        return x\n",
    "    for i in range(max_iter):\n",
    "        Ap = Av_func(p)\n",
    "        pAp = torch.dot(p, Ap)\n",
    "        if pAp <= 0:\n",
    "            raise RuntimeError(\"p^T A p <= 0\")\n",
    "        alpha = rr_old / pAp\n",
    "        x = x + alpha * p\n",
    "        r = r - alpha * Ap\n",
    "        rr_new = torch.dot(r, r)\n",
    "        res_norm = torch.sqrt(rr_new)\n",
    "        if verbose and i % 50 == 0:\n",
    "            print(f\"CG Iter {i}, Residual: {res_norm.item():.6f}\")\n",
    "        if res_norm < tol:\n",
    "            if verbose:\n",
    "                print(f\"Converged at iter {i}, Residual: {res_norm.item():.6f}\")\n",
    "            break\n",
    "        beta = rr_new / rr_old\n",
    "        p = r + beta * p\n",
    "        rr_old = rr_new\n",
    "    return x\n",
    "\n",
    "def solve_W_subproblem(x_inverse, X_mat, Y_mat, Z, U, rho, W_init, tol=1e-6, max_cg_iter=1000, verbose=False):\n",
    "    #x batch* 64, 60 w 60, 60  [60, 180]    [60, 120] [120, 60]\n",
    "    # Y batch * 64, 60 or 180\n",
    "    n = 60\n",
    "    shape_w = W_init.shape\n",
    "    R = Z - U\n",
    "    # b = X^T Y + rho R\n",
    "    XTY = (X_mat.T @ Y_mat) \n",
    "    b_mat = XTY + rho * R\n",
    "    b_flat = b_mat.flatten()\n",
    "\n",
    "    def Av_func(W_flat):\n",
    "        W_mat = W_flat.reshape(shape_w)\n",
    "        XW = X_mat @ W_mat  \n",
    "        XTXW = X_mat.T @ XW\n",
    "        AW = XTXW + rho * W_mat\n",
    "        return AW.flatten()\n",
    "\n",
    "    W0_flat = W_init.flatten()\n",
    "    #W_flat_sol = conjugate_gradient(Av_func, b_flat, W0_flat, tol=tol, max_iter=max_cg_iter, verbose=verbose)\n",
    "    W_flat_sol = x_inverse @ b_mat\n",
    "    W_sol = W_flat_sol.reshape(shape_w)\n",
    "    return W_sol\n",
    "\n",
    "W = W.to(device)\n",
    "Z = Z.to(device)\n",
    "U = U.to(device)\n",
    "\n",
    "for it in range(max_iter):\n",
    "    # W-update\n",
    "    I_n = torch.eye(X_reshape.shape[1], device=X_reshape.device)\n",
    "    x_inverse = torch.inverse(X_reshape.T@X_reshape + rho * I_n)\n",
    "    W = solve_W_subproblem(x_inverse, X_reshape, Y_reshape, Z, U, rho, W, tol=W_cg_tol, max_cg_iter=W_cg_max_iter, verbose=False)\n",
    "\n",
    "    # Z-update\n",
    "    Z = Z_subproblem_update(W, U, lambda_1, lambda_2, rho, Z, delta=delta, lr=Z_lr, inner_iter=Z_inner_iter)\n",
    "\n",
    "    # U-update\n",
    "    U = U + (W - Z)\n",
    "\n",
    "    primal_res = torch.norm(W - Z, p='fro')\n",
    "    res = torch.norm(X_reshape @ W - Y_reshape, p='fro')\n",
    "    if it % 10 == 0:\n",
    "        print(f\"Iter {it}, res: {res.item():.6f}\")\n",
    "        print(f\"Iter {it}, primal_res: {primal_res.item():.6f}\")\n",
    "\n",
    "    if primal_res.item() < 1e-6:\n",
    "        print(\"Converged!\")\n",
    "        break\n",
    "\n",
    "print(\"Optimization Finished. Final primal residual:\", primal_res.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mix-SwinIR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
